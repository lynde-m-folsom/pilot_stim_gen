{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "#import spacy \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from anagram_utils import (\n",
    "    mk_dict_from_wordnet_for_length,\n",
    "    remove_from_dict,\n",
    "    get_word_frequencies,\n",
    "    sort_words_by_frequency,\n",
    "    get_top_n_words,\n",
    "    shuffle_list,\n",
    "    reformat_sorted_wordlist,\n",
    "    check_for_word,\n",
    "    check_for_doubles,\n",
    "    check_for_same,\n",
    "    find_valid_words\n",
    ")\n",
    "nltk.download(\"wordnet\") \n",
    "#import spacy.cli\n",
    "#spacy.cli.download('en_core_web_sm')\n",
    "#nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03148bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2310 words of length 4\n",
      "found 4095 words of length 5\n",
      "found 6258 words of length 6\n",
      "found after removing curse words 2307 words of length 4\n",
      "found after removing curse words 4095 words of length 5\n",
      "found after removing curse words 6258 words of length 6\n",
      "found after removing other words 2304words of length 4\n",
      "found after removing other words 4091words of length 5\n",
      "found after removing other words 6252words of length 6\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "wordlengths = [4,5,6] # set the lengths outside of the loop bc we use them later too \n",
    "for wordlength in wordlengths:\n",
    "    word_dict[wordlength] = mk_dict_from_wordnet_for_length(wordlength)\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# define our curse words or other words to remove\n",
    "curse_words = [\"shit\", \"piss\", \"fuck\", \"cunt\", \"cocksucker\", \"motherfucker\", \"tits\"] #rip george carlin\n",
    "other_words_to_remove = [\"jesus\", \"george\", \"john\", 'james', 'york', 'david', 'google', 'robert', 'thomas','kill','trump', 'stupid', 'centre' ]\n",
    "# use the remove from dict func to take those ones out\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], curse_words)\n",
    "    print(f'found after removing curse words {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], other_words_to_remove)\n",
    "    print(f'found after removing other words {len(word_dict[wordlength])}words of length {wordlength}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54263f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a function that will see if a word is in the dictionary, if it is, print \"___ is in the dictionary\" if not, print \"____ is not in the dictionary\", then check if it is in the word list and say \"___ is in the word list\" or \"___ is not in the word list\" \n",
    "# def check_word(word, word_dict):\n",
    "#     if word in word_dict:\n",
    "#         print(f'{word} is in the dictionary')\n",
    "#     else:\n",
    "#         print(f'{word} is not in the dictionary')\n",
    "#     if word in full_list:\n",
    "#         print(f'{word} is in the word list')\n",
    "#     else:\n",
    "#         print(f'{word} is not in the word list')\n",
    "#     return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71eeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = get_word_frequencies(word_dict) # we get the word frequencies\n",
    "sorted_wordlist = sort_words_by_frequency(word_frequencies) # we sort the dictionary by frequency\n",
    "sorted_wordlist = reformat_sorted_wordlist(sorted_wordlist) # we reformat the dictionary for the get_top_n_words function\n",
    "# make your subset list of words for the anagrams\n",
    "number_of_anagrams_per_word_length = 200\n",
    "full_list = get_top_n_words(sorted_wordlist, number_of_anagrams_per_word_length)\n",
    "# kewl so lets make a list of those words but shuffled letters ie our anagrams\n",
    "shuffled_list = shuffle_list(full_list) # we take the list and make a new list of each word's letters shuffled (anagram)\n",
    "# and for the next few things it's gonna help to also have a df of these two lits named root and shuffled\n",
    "cat_full_list = pd.DataFrame({\n",
    "    \"root\" : full_list,\n",
    "    \"shuffled\" : shuffled_list,\n",
    "    # id collumn which is a string of the index + 1 so that it's 3 digits long\n",
    "    \"id\" : [str(i+1).zfill(3) for i in range(len(full_list))]    \n",
    "})\n",
    "cat_full_list= check_for_doubles(cat_full_list) # we check for any words that are the same in the root and shuffled columns \n",
    "cat_full_list= check_for_same(cat_full_list) # we check for any duplicates in the shuffled coll and reshuffle them\n",
    "## need to make a function that checks that words that are \"shuffled\" do not exist in the dictionary, then shuffle if they are\n",
    "cat_full_list = check_for_word(cat_full_list, word_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e4efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except is in the WordNet corpus\n",
      "[Lemma('demur.v.01.except'), Lemma('exclude.v.01.except')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def check_word_in_wordnet(word):\n",
    "    if wn.synsets(word):\n",
    "        print(f'{word} is in the WordNet corpus')\n",
    "    else:\n",
    "        print(f'{word} is not in the WordNet corpus')\n",
    "\n",
    "check_word_in_wordnet('except')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b85e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nets', 'nest', 'tens', 'sent']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_english_word(word):\n",
    "    \"\"\"\n",
    "    Check if a word exists in WordNet's dictionary\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the word exists in WordNet, False otherwise\n",
    "    \"\"\"\n",
    "    # Convert to lowercase for consistency\n",
    "    word = word.lower().strip()\n",
    "    \n",
    "    # Check if the word exists in WordNet\n",
    "    return len(wn.synsets(word)) > 0\n",
    "\n",
    "def get_all_permutations(word):\n",
    "    return [''.join(p) for p in itertools.permutations(word)]\n",
    "\n",
    "def get_all_permissible_words(input_word):\n",
    "    perms = get_all_permutations(input_word)\n",
    "    return [word for word in perms if is_english_word(word)]\n",
    "\n",
    "get_all_permissible_words('nets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3df4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # then check agasint the word_dict and remove them from the permutations list if they are in the dictionary\n",
    "# valid_words_gram = [find_valid_words(word, word_dict) for word in shuffled_list] # we find the valid words for each anagram in the shuffled list\n",
    "# valid_words = [list(set(words)) for words in valid_words_gram] # we remove any duplicates in the valid words list and set to just the one list\n",
    "\n",
    "\n",
    "# # from a shuffled word list, we if a word is not in the dictionary then we add it to a permutations list\n",
    "# def find_invalid_perms(word, word_dict):\n",
    "#     permutations = [\"\".join(i) for i in itertools.permutations(word)]\n",
    "#     invalid_perms = []\n",
    "#     for candidate in permutations:\n",
    "#         if candidate not in word_dict:\n",
    "#             invalid_perms.append(candidate)\n",
    "#     return invalid_perms\n",
    "\n",
    "# invalid_perms = [find_invalid_perms(word, word_dict) for word in shuffled_list]\n",
    "\n",
    "# okay now lets use the get_all_permissible_words function to get the valid words for each anagram\n",
    "valid_words_gram = [get_all_permissible_words(word) for word in shuffled_list]\n",
    "valid_words_gram = [list(set(words)) for words in valid_words_gram]\n",
    "\n",
    "# # remove the valid_words from the permutations list\n",
    "# print(len(invalid_perms))\n",
    "# for word in valid_words:\n",
    "#     if word in invalid_perms:\n",
    "#         invalid_perms.remove(word)\n",
    "\n",
    "# # remove duplicates in eachline of the invalid perms list\n",
    "# invalid_perms = [list(set(words)) for words in invalid_perms]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bccc3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_words_gram = [find_valid_words(word, word_dict) for word in shuffled_list] # we find the valid words for each anagram in the shuffled list\n",
    "# valid_words = [list(set(words)) for words in valid_words_gram] # we remove any duplicates in the valid words list and set to just the one list\n",
    "# cat_full_list['valid_words'] = valid_words # we add the valid words to the df\n",
    "# cat_full_list=check_for_word(cat_full_list, word_dict) # we check for any words that are the same in the root and shuffled columns\n",
    "\n",
    "# lets do this again but witht he valid_words_with_newfunc\n",
    "valid_words_gram = [get_all_permissible_words(word) for word in shuffled_list] # we find the valid words for each anagram in the shuffled list\n",
    "valid_words_gram = [list(set(words)) for words in valid_words_gram] # we remove any duplicates in the valid words list and set to just the one list\n",
    "cat_full_list['valid_words'] = valid_words_gram # we add the valid words to the df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "888be201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetA has 150 words\n",
      "SetB has 150 words\n",
      "SetC has 150 words\n",
      "SetD has 150 words\n"
     ]
    }
   ],
   "source": [
    "# function to use the root in the df to assign it's string length in a new column of the same def\n",
    "def get_length(row):\n",
    "    return len(row['root']) \n",
    "\n",
    "# apply the function to the df\n",
    "cat_full_list['length'] = cat_full_list.apply(get_length, axis=1)\n",
    "\n",
    "# function to first group by the length in a DF then randomly assign to a group a through d \n",
    "def group_by_length(df):\n",
    "    #\n",
    "    if len(df) % 4 != 0:\n",
    "        raise \"Error in distributing between 4 groups, please revise counts\"\n",
    "    # group by the length of the word\n",
    "    grouped = df.groupby('length')\n",
    "    # make a list of the groups\n",
    "    groups = [group for name, group in grouped]\n",
    "    # make a list of the group names\n",
    "    group_names = ['SetA', 'SetB', 'SetC', 'SetD']\n",
    "    # calculate number of words per group\n",
    "    num_words_per_group = len(df) // len(group_names)\n",
    "    # make a dictionary of the group names and the groups\n",
    "    group_dict = dict(zip(group_names, groups))\n",
    "    # make a list of the group names for each word in the df don't exceed the num words per group\n",
    "    group_list = [] # make an empty list to append to\n",
    "    # loop through the group names until the number of words in the group is equal to the number of words per group\n",
    "    for group_name in group_names:\n",
    "        for i in range(num_words_per_group):\n",
    "            group_list.append(group_name)\n",
    "    # assign the group list to the df\n",
    "    df['Set'] = group_list\n",
    "    return df\n",
    "\n",
    "# apply the function to the df\n",
    "cat_full_list = group_by_length(cat_full_list)\n",
    "\n",
    "# check that each group in the df has the same number of words\n",
    "grouped = cat_full_list.groupby('Set')\n",
    "for name, group in grouped:\n",
    "    print(f'{name} has {len(group)} words')\n",
    "\n",
    "#### troublesome function ***** now we create 4 unique set runs for each set and call that col setrun and it uses the val of set and adds it's number\n",
    "### so that we have 4 unique runs for each set, this would then mean our final js stim  would have 16 unique runs, 4 for each set, and a file that ought to be len(id) * 16\n",
    "\n",
    "def set_run(df):\n",
    "    # group by set\n",
    "    grouped = df.groupby('Set')\n",
    "    # for each group, shuffle and assign it to a set run A1, A2, A3, A4, B1, B2, B3, B4, etc.\n",
    "    set_runs_list = []\n",
    "    # loop through the groups four times, each time shuffle the order and then append taht list to the set runs\n",
    "    for name, group in grouped:\n",
    "        for i in range(4):\n",
    "            # take the id and shuffle the order (but not the contents, just the order) and append to the set runs list\n",
    "            set_runs_list.append(group['id'].sample(frac=1).tolist())\n",
    "    # make a new column in the df and assign the set runs list to it\n",
    "    df['set_run'] = set_runs_list\n",
    "    return df\n",
    "#save the cat_full_list to a csv\n",
    "cat_full_list.to_csv('cat_full_list.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "657b2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "{'id': '90', 'type': '6', 'anagram': 'rttmea', 'correct': 'matter', 'valid': \"['matter']\", 'set': 'SetA', 'setRun': 'SetA1'}\n"
     ]
    }
   ],
   "source": [
    "full_list = pd.read_csv('cat_full_list.csv') #def gonna need to be optimized\n",
    "\n",
    "target_sets = full_list['Set'].unique()\n",
    "nlists = 4\n",
    "rand_full_list = None \n",
    "\n",
    "for target_set in target_sets:\n",
    "    for list_num in range(1, nlists+1):\n",
    "        set_list_name = f'{target_set}{list_num}'\n",
    "        target_list = full_list[full_list['Set'] == target_set]\n",
    "        # randoly shuffle order of target list\n",
    "        target_list = target_list.sample(frac=1).reset_index(drop=True)\n",
    "        target_list['SetRun'] = set_list_name\n",
    "        if rand_full_list is None:\n",
    "            rand_full_list = target_list\n",
    "        else:\n",
    "            rand_full_list = pd.concat([rand_full_list, target_list])\n",
    "\n",
    "\n",
    "def format_js_stimuli(df):\n",
    "    js_stimuli = []\n",
    "    for idx, row in df.iterrows():\n",
    "        js_entry = {\n",
    "                \"id\": str(row['id']),\n",
    "                \"type\": str(row['length']),\n",
    "                \"anagram\": row['shuffled'],\n",
    "                \"correct\": row['root'],\n",
    "                \"valid\": row['valid_words'],\n",
    "                \"set\": row['Set'],\n",
    "                \"setRun\": row['SetRun'],\n",
    "            }\n",
    "        js_stimuli.append(js_entry) \n",
    "    return js_stimuli\n",
    "\n",
    "rand_full_list_json = format_js_stimuli(rand_full_list)\n",
    "print(len(rand_full_list_json))\n",
    "print(rand_full_list_json[0])\n",
    "\n",
    "with open('rand_full_list.json', 'w') as f:\n",
    "    json.dump(rand_full_list_json, f, indent=4)\n",
    "\n",
    "with open('rand_full_list.json', 'r') as f:\n",
    "    rand_full_list_json_lines = f.readlines()\n",
    "\n",
    "first_line = \"let trial_objects = [\\n\" \n",
    "last_line = \"];\"\n",
    "rand_full_list_json_lines[0] = first_line\n",
    "rand_full_list_json_lines[-1] = last_line\n",
    "\n",
    "\n",
    "with open('stimuli.js', 'w') as f:\n",
    "    f.writelines(rand_full_list_json_lines)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
